# SGR Deep Research → OpenWebUI Proxy

**Статус:** Тестовый / экспериментальный  
**Версия:** 0.0.1

Минималистичный прокси-сервер для интеграции [SGR Deep Research](https://github.com/vamplabAI/sgr-deep-research) с [Open WebUI](https://github.com/open-webui/open-webui).

---

## Зачем это нужно?

SGR Deep Research — это reasoning-агент с инструментами для поиска, извлечения контента и уточняющих вопросов. Он работает через OpenAI-совместимое API, но его вывод — это JSON-лог с шагами рассуждений, статусами инструментов и финальными ответами.

Open WebUI ожидает обычный стриминг текста, как от ChatGPT/Claude, и умеет сворачивать блоки `<thinking>` в панели «Thought for X seconds».

Этот прокси:
- Стримит JSON-reasoning от SGR внутри `<thinking>…</thinking>` → в Open WebUI он попадает в сворачиваемую панель «Рассуждение».
- Извлекает финальный ответ или уточняющие вопросы и отдаёт их как обычный текст ассистента (видимый пользователю).
- Отслеживает agent_id для продолжения диалога при уточнениях, чтобы Open WebUI мог корректно передавать ответы пользователя в тот же SGR-агент.

---

## Статус разработки

⚠️ **Пока не допилят нативную поддержку в SGR или Open WebUI** — это временное решение для тестирования.

- Работает со стримом reasoning в `<thinking>`.
- Поддерживает `clarificationtool` (уточняющие вопросы).
- Поддерживает `finalanswertool` (финальные ответы).


Ограничения:
- Некоторые reasoning-шаги могут дублироваться (зависит от реализации SGR).
- Если SGR настроен с длинным `max_tokens`, reasoning может быть очень подробным.
- Open WebUI иногда создаёт дополнительные панели «Thought for…» из-за внутренних фильтров.

---

## Установка и запуск

### 1. Требования

- Python 3.12+
- Запущенный SGR Deep Research сервер (например, `http://localhost:8010`)
- Open WebUI (локально или в Docker)

### 2. Установка зависимостей

pip install -r requirements.txt



### 3. Настройка переменных окружения (опционально)

export SGR_BASE_URL=http://localhost:8010

export OPENAI_MODEL_NAME=sgr_agent

export PROXY_PORT=8000


### 4. Запуск прокси

python app.py


Прокси будет слушать на `http://0.0.0.0:8000`.

### 5. Настройка Open WebUI

В настройках Open WebUI (Admin Panel → Settings → Connections → OpenAI API):

- **Base URL**: `http://localhost:8000/v1`
- **API Key**: любая строка (прокси её игнорирует)
- **Model**: `sgr_agent` (или любое имя модели из `/v1/models` вашего SGR)

Сохраните и выберите модель в чате.

---

## Как это работает?

1. Open WebUI отправляет `POST /v1/chat/completions` с историей чата.
2. Прокси проверяет последнее сообщение ассистента на наличие маркера `<!--sgr_agent_id:...-->`:
   - Если есть → продолжает сессию этого агента (пользователь отвечает на уточняющий вопрос).
   - Если нет → стартует нового агента.
3. SGR стримит JSON-лог с reasoning_steps и вызовами инструментов.
4. Прокси оборачивает весь стрим в `<thinking>…</thinking>` → Open WebUI показывает сворачиваемую панель.
5. Когда SGR заканчивает:
   - Если есть `finalanswertool` → прокси отдаёт финальный ответ без маркера.
   - Если есть `clarificationtool` → прокси отдаёт вопросы + невидимый маркер `agent_id` для продолжения.

---


---

## FAQ

**Q: Почему reasoning в `<thinking>` появляется сразу, а не постепенно?**  
A: SGR стримит JSON-токены, но вся структура формируется постепенно. Open WebUI рендерит `<thinking>` как один блок; для по-токенного стрима нужна доработка SGR или фронтенда.

**Q: Почему иногда две панели «Thought for…»?**  
A: Это артефакт reasoning-фильтров Open WebUI, которые тоже пытаются обработать сырой вывод. В будущем это должно быть исправлено.

**Q: Можно ли использовать на проде?**  
A: Нет — это временный workaround для тестирования. Ждите официальной интеграции.

---

## Лицензия

MIT

---

##  Благодарности

- [SGR Deep Research](https://github.com/vamplabAI/sgr-deep-research)
- [Open WebUI](https://github.com/open-webui/open-webui)

Если у вас есть идеи или фиксы — welcome в Issues/PRs!



